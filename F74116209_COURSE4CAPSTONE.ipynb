{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Capstone Example\n",
    "\n",
    "Last week's notebook discussed a cummulative project that would be used as a test of knowledge from this series of courses.\n",
    "This notebook will serve as a reference point for you while you work on said project. Included in this notebook is a set of answers to your tasks, based on a set dataset. Make sure in your final submission you are using a different dataset! \n",
    "\n",
    "You will be working on 4 tasks:\n",
    "1. __Data Processing__ \n",
    "2. __Classification__ \n",
    "3. __Regression__ \n",
    "4. __Recommender Sytstems__\n",
    "\n",
    "These tasks are each representative of one of the courses in the series. So if you need help with any one of these tasks, be sure to look back at those courses for reference! Along with the previous courses, there will be checkpoints with given solutions so you can check to make sure you are headed in the right direction. ___Good Luck!___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Description\n",
    "The dataset analyzed in this project is titled \"Amazon Musical Instruments Reviews.\" It comprises 904,004 rows and 15 columns, providing detailed information about customer reviews for products listed on Amazon. A comprehensive description of each column is provided below:\n",
    "1. __marketplace:__ The region or country (e.g., US) where the review was submitted.\n",
    "2. __customer_id:__ A unique identifier for the customer who submitted the review.\n",
    "3. __review_id:__ A unique identifier for the review.\n",
    "4. __product_id:__ A unique identifier for the product.\n",
    "5. __product_parent:__ An internal Amazon product identifier, used to group products under a parent category or product line.\n",
    "6. __product_title:__ The title or name of the product, providing a description of the item.\n",
    "7. __product_category:__ The category of the product, such as \"Musical Instruments\" in this dataset.\n",
    "8. __star_rating:__ The star rating given by the user to the product, ranging from 1 to 5 stars.\n",
    "9. __helpful_votes:__ The number of votes the review received as helpful from other users.\n",
    "10. __total_votes:__ The total number of votes the review received, including both helpful and not helpful votes.\n",
    "11. __vine:__ Indicates whether the review was part of Amazon's Vine Program.\n",
    "12. __verified_purchase:__ Indicates whether the review came from a verified purchase.\n",
    "13. __review_headline:__ The headline of the review, summarizing its main points.\n",
    "14. __review_body:__ The full textual content of the review, describing the user's experience and feedback about the product.\n",
    "15. __review_date:__ The date the review was submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Processing\n",
    "\n",
    "## The Data\n",
    "\n",
    "For this final project you will be doing your work on a dataset of your choice. For reference, an example with checkpoint answers will be included. This example will be an amazon dataset, which does not need any cleaning before proper analysis. This dataset in particular can be found [here.](https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Home_Improvement_v1_00.tsv.gz)\n",
    "This dataset is a set of Home Improvement Product reviews on amazon. It is a rather large dataset, so our computation might take slightly longer than normal.\n",
    "\n",
    "### First Step: Imports\n",
    "\n",
    "In the next cell we will give you all of the imports you should need to do your project. Feel free to add more if you would like, but these should be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "import numpy\n",
    "import scipy.optimize\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "from nltk.stem.porter import PorterStemmer # Stemming\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Read the data and Fill your dataset\n",
    "\n",
    "Take care of int casting the votes and rating. Also __add this bit of code__ to your for loop, taking off the outer \" \":\n",
    "\n",
    "\" d['verified_purchase'] = d['verified_purchase'] == 'Y' \"\n",
    "\n",
    "This simple makes the verified purchase column be strictly true/false values rather than Y/N strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 904004 entries, 0 to 904003\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   marketplace        904004 non-null  object\n",
      " 1   customer_id        904004 non-null  int64 \n",
      " 2   review_id          904004 non-null  object\n",
      " 3   product_id         904004 non-null  object\n",
      " 4   product_parent     904004 non-null  int64 \n",
      " 5   product_title      904003 non-null  object\n",
      " 6   product_category   904004 non-null  object\n",
      " 7   star_rating        904004 non-null  int64 \n",
      " 8   helpful_votes      904004 non-null  int64 \n",
      " 9   total_votes        904004 non-null  int64 \n",
      " 10  vine               904004 non-null  object\n",
      " 11  verified_purchase  904004 non-null  object\n",
      " 12  review_headline    903998 non-null  object\n",
      " 13  review_body        903941 non-null  object\n",
      " 14  review_date        903996 non-null  object\n",
      "dtypes: int64(5), object(10)\n",
      "memory usage: 103.5+ MB\n",
      "None\n",
      "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
      "0          US     45610553   RMDCHWD0Y5OZ9  B00HH62VB6       618218723   \n",
      "1          US     14640079   RZSL0BALIYUNU  B003LRN53I       986692292   \n",
      "2          US      6111003   RIZR67JKUDBI0  B0006VMBHI       603261968   \n",
      "3          US      1546619  R27HL570VNL85F  B002B55TRG       575084461   \n",
      "4          US     12222213  R34EBU9QDWJ1GD  B00N1YPXW2       165236328   \n",
      "\n",
      "                                       product_title     product_category  \\\n",
      "0  AGPtekÂ® 10 Isolated Output 9V 12V 18V Guitar P...  Musical Instruments   \n",
      "1         Sennheiser HD203 Closed-Back DJ Headphones  Musical Instruments   \n",
      "2                   AudioQuest LP record clean brush  Musical Instruments   \n",
      "3      Hohner Inc. 560BX-BF Special Twenty Harmonica  Musical Instruments   \n",
      "4        Blue Yeti USB Microphone - Blackout Edition  Musical Instruments   \n",
      "\n",
      "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
      "0            3              0            1    N                 N   \n",
      "1            5              0            0    N                 Y   \n",
      "2            3              0            1    N                 Y   \n",
      "3            5              0            0    N                 Y   \n",
      "4            5              0            0    N                 Y   \n",
      "\n",
      "                                     review_headline  \\\n",
      "0                                        Three Stars   \n",
      "1                                         Five Stars   \n",
      "2                                        Three Stars   \n",
      "3  I purchase these for a friend in return for pl...   \n",
      "4                                         Five Stars   \n",
      "\n",
      "                                         review_body review_date  \n",
      "0        Works very good, but induces ALOT of noise.  2015-08-31  \n",
      "1             Nice headphones at a reasonable price.  2015-08-31  \n",
      "2                       removes dust. does not clean  2015-08-31  \n",
      "3  I purchase these for a friend in return for pl...  2015-08-31  \n",
      "4                            This is an awesome mic!  2015-08-31  \n"
     ]
    }
   ],
   "source": [
    "# Path to the uploaded file\n",
    "file_path = 'amazon_reviews_us_Musical_Instruments_v1_00.tsv'\n",
    "\n",
    "# Extract and load the complete data\n",
    "df = pd.read_csv(file_path, sep='\\t', on_bad_lines='skip')\n",
    "\n",
    "# Display dataset info and the first few rows\n",
    "print(df.info())  # Summary of the dataset\n",
    "print(df.head())  # Display the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['verified_purchase'] = df['verified_purchase'] == 'Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column before dropping:\n",
      "marketplace           0\n",
      "customer_id           0\n",
      "review_id             0\n",
      "product_id            0\n",
      "product_parent        0\n",
      "product_title         1\n",
      "product_category      0\n",
      "star_rating           0\n",
      "helpful_votes         0\n",
      "total_votes           0\n",
      "vine                  0\n",
      "verified_purchase     0\n",
      "review_headline       6\n",
      "review_body          63\n",
      "review_date           8\n",
      "dtype: int64\n",
      "\n",
      "Missing values per column after dropping:\n",
      "marketplace          0\n",
      "customer_id          0\n",
      "review_id            0\n",
      "product_id           0\n",
      "product_parent       0\n",
      "product_title        0\n",
      "product_category     0\n",
      "star_rating          0\n",
      "helpful_votes        0\n",
      "total_votes          0\n",
      "vine                 0\n",
      "verified_purchase    0\n",
      "review_headline      0\n",
      "review_body          0\n",
      "review_date          0\n",
      "dtype: int64\n",
      "\n",
      "Cleaned dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 903926 entries, 0 to 904003\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   marketplace        903926 non-null  object\n",
      " 1   customer_id        903926 non-null  int64 \n",
      " 2   review_id          903926 non-null  object\n",
      " 3   product_id         903926 non-null  object\n",
      " 4   product_parent     903926 non-null  int64 \n",
      " 5   product_title      903926 non-null  object\n",
      " 6   product_category   903926 non-null  object\n",
      " 7   star_rating        903926 non-null  int64 \n",
      " 8   helpful_votes      903926 non-null  int64 \n",
      " 9   total_votes        903926 non-null  int64 \n",
      " 10  vine               903926 non-null  object\n",
      " 11  verified_purchase  903926 non-null  bool  \n",
      " 12  review_headline    903926 non-null  object\n",
      " 13  review_body        903926 non-null  object\n",
      " 14  review_date        903926 non-null  object\n",
      "dtypes: bool(1), int64(5), object(9)\n",
      "memory usage: 104.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values before dropping\n",
    "print(\"Missing values per column before dropping:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Check for missing values after dropping\n",
    "print(\"\\nMissing values per column after dropping:\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "\n",
    "# Display the cleaned dataset summary\n",
    "print(\"\\nCleaned dataset info:\")\n",
    "print(df_cleaned.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this setup properly, you __should__ shuffle your data (which you should do in your submission), but the checkpoint values would change so for the sake of this example we will ___not___ shuffle the data.\n",
    "\n",
    "### TODO 2: Split the data into a Training and Testing set\n",
    "\n",
    "Have Training be the first 80%, and testing be the remaining 20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723140 180786\n"
     ]
    }
   ],
   "source": [
    "def split_data_randomly(data, train_ratio=0.8, random_state=None):\n",
    "    \"\"\"\n",
    "    Randomly splits the dataset into training and testing sets.\n",
    "    :param data: The original dataset (DataFrame).\n",
    "    :param train_ratio: The ratio of the data to be used as the training set (default is 80%).\n",
    "    :param random_state: The seed for random number generation (optional, ensures reproducibility).\n",
    "    :return: Two DataFrames - the training set and the testing set.\n",
    "    \"\"\"\n",
    "    # Use train_test_split to split the dataset based on the specified train/test ratio\n",
    "    train_data, test_data = train_test_split(\n",
    "        data, test_size=(1 - train_ratio), random_state=random_state\n",
    "    )\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = split_data_randomly(df_cleaned, train_ratio=0.8, random_state=42)\n",
    "\n",
    "\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now delete your dataset\n",
    "You don't want any of your answers to come from your original dataset any longer, but rather your Training Set, this will help you to not make any mistakes later on, especialy when referencing the checkpoint solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df\n",
    "del df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Extracting Basic Statistics\n",
    "\n",
    "Next you need to answer some questions through any means (i.e. write a function or just find the answer) all based on the __Training Set:__\n",
    "1. What is the __average rating__?\n",
    "2. What fraction of reviews are from __verified purchases__?\n",
    "3. How many __total users__ are there?\n",
    "4. How many __total items__ are there?\n",
    "5. What fraction of reviews have __5-star ratings__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Average Rating: 4.25\n",
      "2. Fraction of Verified Purchases: 86.38%\n",
      "3. Total Users: 481417\n",
      "4. Total Items: 111099\n",
      "5. Fraction of 5-Star Ratings: 63.32%\n"
     ]
    }
   ],
   "source": [
    "# 1. Function to calculate the average star rating\n",
    "def calculate_average_rating(data):\n",
    "    \"\"\"\n",
    "    Calculate the average star rating from the dataset.\n",
    "    :param data: The dataset (DataFrame) containing the 'star_rating' column.\n",
    "    :return: The mean of the 'star_rating' column.\n",
    "    \"\"\"\n",
    "    return data['star_rating'].mean()\n",
    "\n",
    "# 2. Function to calculate the fraction of verified purchases\n",
    "def calculate_verified_fraction(data):\n",
    "    \"\"\"\n",
    "    Calculate the fraction of reviews that are verified purchases.\n",
    "    :param data: The dataset (DataFrame) containing the 'verified_purchase' column.\n",
    "    :return: The mean value of the 'verified_purchase' column (True/False as 1/0).\n",
    "    \"\"\"\n",
    "    return data[\"verified_purchase\"].mean()\n",
    "\n",
    "# 3. Function to calculate the total number of unique users\n",
    "def calculate_total_users(data):\n",
    "    \"\"\"\n",
    "    Calculate the total number of unique users who submitted reviews.\n",
    "    :param data: The dataset (DataFrame) containing the 'customer_id' column.\n",
    "    :return: The count of unique customer IDs.\n",
    "    \"\"\"\n",
    "    return data['customer_id'].nunique()\n",
    "\n",
    "# 4. Function to calculate the total number of unique products\n",
    "def calculate_total_items(data):\n",
    "    \"\"\"\n",
    "    Calculate the total number of unique products reviewed in the dataset.\n",
    "    :param data: The dataset (DataFrame) containing the 'product_id' column.\n",
    "    :return: The count of unique product IDs.\n",
    "    \"\"\"\n",
    "    return data['product_id'].nunique()\n",
    "\n",
    "# 5. Function to calculate the fraction of 5-star ratings\n",
    "def calculate_five_star_fraction(data):\n",
    "    \"\"\"\n",
    "    Calculate the fraction of reviews that have a 5-star rating.\n",
    "    :param data: The dataset (DataFrame) containing the 'star_rating' column.\n",
    "    :return: The proportion of 5-star ratings in the dataset.\n",
    "    \"\"\"\n",
    "    return (data['star_rating'] == 5).sum() / len(data)\n",
    "\n",
    "# Call each function to compute metrics based on the training dataset\n",
    "average_rating = calculate_average_rating(train_data)\n",
    "verified_fraction = calculate_verified_fraction(train_data)\n",
    "total_users = calculate_total_users(train_data)\n",
    "total_items = calculate_total_items(train_data)\n",
    "five_star_fraction = calculate_five_star_fraction(train_data)\n",
    "\n",
    "# Display the results in a formatted output\n",
    "print(f\"1. Average Rating: {average_rating:.2f}\")\n",
    "print(f\"2. Fraction of Verified Purchases: {verified_fraction:.2%}\")\n",
    "print(f\"3. Total Users: {total_users}\")\n",
    "print(f\"4. Total Items: {total_items}\")\n",
    "print(f\"5. Fraction of 5-Star Ratings: {five_star_fraction:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Classification\n",
    "\n",
    "Next you will use our knowledge of classification to extract features and make predictions based on them. Here you will be using a Logistic Regression Model, keep this in mind so you know where to get help from.\n",
    "\n",
    "### TODO 1: Define the feature function\n",
    "\n",
    "This implementation will be based on the __star rating__ and the ___length___ of the __review body__. Hint: Remember the offset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(data):\n",
    "    \"\"\"\n",
    "    Define the feature extraction function based on review body length and star rating.\n",
    "    :param data: The original dataset (DataFrame).\n",
    "    :return: A feature matrix X and a target array y.\n",
    "    \"\"\"\n",
    "    # Calculate the length of the review body\n",
    "    review_length = data['review_body'].fillna('').apply(len).values\n",
    "    # Extract the star ratings as features\n",
    "    star_rating = data['star_rating'].values\n",
    "    # Combine the features into a single feature matrix\n",
    "    X = np.vstack((review_length, star_rating)).T\n",
    "    # Extract the target variable (whether the purchase is verified or not)\n",
    "    y = data['verified_purchase'].astype(int).values\n",
    "    return X, y\n",
    "\n",
    "# Generate features and target variables for the training dataset\n",
    "X_train, y_train = create_features(train_data)\n",
    "# Generate features and target variables for the testing dataset\n",
    "X_test, y_test = create_features(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Fit your model\n",
    "\n",
    "1. Create your __Feature Vector__ based on your feature function defined above. \n",
    "2. Create your __Label Vector__ based on the \"verified purchase\" column of your training set.\n",
    "3. Define your model as a __Logistic Regression__ model.\n",
    "4. Fit your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3: Compute Accuracy of Your Model\n",
    "\n",
    "1. Make __Predictions__ based on your model.\n",
    "2. Compute the __Accuracy__ of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 86.37%\n"
     ]
    }
   ],
   "source": [
    "# Compute the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4: Finding the Balanced Error Rate\n",
    "\n",
    "1. Compute __True__ and __False Positives__\n",
    "2. Compute __True__ and __False Negatives__\n",
    "3. Compute __Balanced Error Rate__ based on your above defined variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Error Rate (BER): 48.12%\n"
     ]
    }
   ],
   "source": [
    "def compute_ber(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the Balanced Error Rate (BER).\n",
    "    :param y_true: The true labels (ground truth).\n",
    "    :param y_pred: The predicted labels.\n",
    "    :return: The Balanced Error Rate (BER).\n",
    "    \"\"\"\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Extract True Negatives (tn), False Positives (fp), False Negatives (fn), and True Positives (tp) from the confusion matrix\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    # Calculate sensitivity (recall for the positive class)\n",
    "    sensitivity = tp / (tp + fn)  \n",
    "    # Calculate specificity (recall for the negative class)\n",
    "    specificity = tn / (tn + fp) \n",
    "    # Compute the Balanced Error Rate\n",
    "    ber = 1 - 0.5 * (sensitivity + specificity)\n",
    "    return ber\n",
    "\n",
    "# Calculate the Balanced Error Rate for the test dataset\n",
    "ber = compute_ber(y_test, y_pred)\n",
    "print(f\"Balanced Error Rate (BER): {ber:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Regression\n",
    "\n",
    "In this section you will start by working though two examples of altering features to further differentiate. Then you will work through how to evaluate a Regularaized model.\n",
    "\n",
    "Lets start by defining a new y vector, specific to our Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg = train_data['star_rating'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Unique Words in a Sample Set\n",
    "\n",
    "We are going to work with a smaller Sample Set here, as stemming on the normal training set will take a very long time. (Feel free to change sampleSet -> trainingSet if you would like to see)\n",
    "\n",
    "1. Count the number of unique words found within the 'review body' portion of the sample set defined below, making sure to __Ignore Punctuation and Capitalization__.\n",
    "2. Count the number of unique words found within the 'review body' portion of the sample set defined below, this time with use of __Stemming,__ __Ignoring Puctuation,__ ___and___ __Capitalization__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN for 1.\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "#GIVEN for 2.\n",
    "wordCountStem = defaultdict(int)\n",
    "stemmer = PorterStemmer() #use stemmer.stem(stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSet = train_data[:2*len(train_data)//10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Words (No Stemming): 72285\n",
      "Unique Words (With Stemming): 53648\n"
     ]
    }
   ],
   "source": [
    "# Count unique words without stemming\n",
    "for review in sampleSet['review_body'].fillna(''):  # Fill missing values with an empty string\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = ''.join([char.lower() if char not in punctuation else ' ' for char in review]).split()\n",
    "    # Count occurrences of each word\n",
    "    for word in words:\n",
    "        wordCount[word] += 1\n",
    "\n",
    "# Count unique words with stemming\n",
    "for review in sampleSet['review_body'].fillna(''):  # Fill missing values with an empty string\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = ''.join([char.lower() if char not in punctuation else ' ' for char in review]).split()\n",
    "    # Apply stemming and count occurrences\n",
    "    for word in words:\n",
    "        stemmed_word = stemmer.stem(word)  # Apply stemming to the word\n",
    "        wordCountStem[stemmed_word] += 1\n",
    "\n",
    "# Calculate the number of unique words in both cases\n",
    "unique_words_no_stemming = len(wordCount)\n",
    "unique_words_with_stemming = len(wordCountStem)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Unique Words (No Stemming): {unique_words_no_stemming}\")\n",
    "print(f\"Unique Words (With Stemming): {unique_words_with_stemming}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Evaluating Classifiers\n",
    "\n",
    "1. Given the feature function and your counts vector, __Define__ your X_reg vector. (This being the X vector, simply labeled for the Regression model)\n",
    "2. __Fit__ your model using a __Ridge Model__ with (alpha = 1.0, fit_intercept = True).\n",
    "3. Using your model, __Make your Predictions__.\n",
    "4. Find the __MSE__ between your predictions and your y_reg vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN FUNCTIONS\n",
    "def feature_reg(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review_body'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in wordSet:\n",
    "            feat[wordId[w]] += 1\n",
    "    return feat\n",
    "\n",
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN COUNTS AND SETS\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "#Note: increasing the size of the dictionary may require a lot of memory\n",
    "words = [x[1] for x in counts[:100]]\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 1.23\n"
     ]
    }
   ],
   "source": [
    "# Define the feature extraction function (based on word frequency)\n",
    "def create_X_y_for_regression(data):\n",
    "    \"\"\"\n",
    "    Create the feature matrix X and target vector y for regression.\n",
    "    :param data: The original dataset (DataFrame).\n",
    "    :return: Feature matrix X and target vector y.\n",
    "    \"\"\"\n",
    "    # Use CountVectorizer to convert the text data into a bag-of-words representation\n",
    "    # Only the top 100 most frequent words are selected as features\n",
    "    vectorizer = CountVectorizer(max_features=100)\n",
    "    \n",
    "    # Transform the 'review_body' column into a feature matrix\n",
    "    # Fill missing values with an empty string before transformation\n",
    "    X = vectorizer.fit_transform(data['review_body'].fillna('')).toarray()\n",
    "    \n",
    "    # Extract the target variable (star ratings)\n",
    "    y = data['star_rating'].values \n",
    "    return X, y\n",
    "\n",
    "# Create the feature matrix X and target vector y for regression\n",
    "X_reg, y_reg = create_X_y_for_regression(train_data)\n",
    "\n",
    "# Initialize the Ridge Regression model\n",
    "ridge_model = Ridge(alpha=1.0, fit_intercept=True)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Ridge Regression model on the training data\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable for the validation set\n",
    "y_pred = ridge_model.predict(X_val)\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE) to evaluate model performance\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you would like to work with this example more in your free time, here are some tips to improve your solution:\n",
    "# 1. Implement a validation pipeline and tune the regularization parameter\n",
    "# 2. Alter the word features (e.g. dictionary size, punctuation, capitalization, stemming, etc.)\n",
    "# 3. Incorporate features other than word features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Recommendation Systems\n",
    "\n",
    "For your final task, you will use your knowledge of simple latent factor-based recommender systems to make predictions. Then evaluating the performance of your predictions.\n",
    "\n",
    "### Starting up\n",
    "\n",
    "The next cell contains some starter code that you will need for your tasks in this section.\n",
    "Notice you are back to using the __trainingSet__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and fill our default dictionaries for our dataset\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "\n",
    "for _, row in train_data.iterrows():\n",
    "    user, item = row[\"customer_id\"], row[\"product_id\"]\n",
    "    reviewsPerUser[user].append(row)\n",
    "    reviewsPerItem[item].append(row)\n",
    "    \n",
    "#Create two dictionaries that will be filled with our rating prediction values\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)\n",
    "\n",
    "#Getting the respective lengths of our dataset and dictionaries\n",
    "N = len(train_data)\n",
    "nUsers = len(reviewsPerUser)\n",
    "nItems = len(reviewsPerItem)\n",
    "\n",
    "#Getting the list of keys\n",
    "users = list(reviewsPerUser.keys())\n",
    "items = list(reviewsPerItem.keys())\n",
    "\n",
    "### You will need to use this list\n",
    "y_rec = train_data['star_rating'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1: Calculate the ratingMean\n",
    "\n",
    "1. Find the __average rating__ of your training set.\n",
    "2. Calculate a __baseline MSE value__ from the actual ratings to the average ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Rating (ratingMean): 4.25\n",
      "Baseline MSE: 1.480544\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Calculate the average rating of the training set\n",
    "ratingMean = train_data['star_rating'].mean()\n",
    "print(f\"Average Rating (ratingMean): {ratingMean:.2f}\")\n",
    "\n",
    "# Step 2: Calculate the baseline MSE (Baseline Mean Squared Error)\n",
    "baseline_predictions = [ratingMean] * len(y_rec)\n",
    "\n",
    "# Step 3: Calculate the baseline MSE\n",
    "baseline_mse = mean_squared_error(y_rec, baseline_predictions)\n",
    "print(f\"Baseline MSE: {baseline_mse:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are defining the functions you will need to optimize your MSE value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN\n",
    "alpha = ratingMean\n",
    "\n",
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item]\n",
    "\n",
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    itemBiases = dict(zip(items, theta[1+nUsers:]))\n",
    "    \n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(row[\"customer_id\"], row[\"product_id\"]) for _, row in train_data.iterrows()]\n",
    "    cost = MSE(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in itemBiases:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "    return cost\n",
    "\n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(train_data)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for _, row in train_data.iterrows():\n",
    "        user, item = row[\"customer_id\"], row[\"product_id\"]\n",
    "        pred = prediction(user, item)\n",
    "        diff = pred - row[\"star_rating\"]\n",
    "        dUserBiases[user] += 2 * diff / N\n",
    "        dItemBiases[item] += 2 * diff / N\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    return numpy.array(dtheta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Optimize\n",
    "\n",
    "1. __Optimize__ your MSE using the scipy.optimize.fmin_1_bfgs_b(\"arguments\") functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.4805442518035832\n",
      "MSE = 1.4694341120672842\n",
      "MSE = 1.4798262523165713\n",
      "MSE = 1.4798261046628873\n",
      "Optimized alpha: 4.2508145034156595\n",
      "Final MSE: 1.480184\n"
     ]
    }
   ],
   "source": [
    "# Initialize values\n",
    "initial_alpha = ratingMean # Initial value for the global bias (alpha), set to the average rating\n",
    "initial_userBiases = np.zeros(nUsers)  # Initialize user biases to 0\n",
    "initial_itemBiases = np.zeros(nItems)  # Initialize item biases to 0\n",
    "initial_theta = np.concatenate([[initial_alpha], initial_userBiases, initial_itemBiases])  # Combine all initial parameters into a single array (theta)\n",
    "\n",
    "# Define the regularization parameter (lambda)\n",
    "lamb = 0.1\n",
    "\n",
    "# Optimize the MSE (Mean Squared Error) using the fmin_l_bfgs_b function\n",
    "optimal_theta, optimal_cost, _ = fmin_l_bfgs_b(\n",
    "    func=cost,  # Cost function to minimize\n",
    "    x0=initial_theta,  # Initial values for optimization\n",
    "    fprime=derivative,  # Derivative of the cost function (gradient)\n",
    "    args=(y_rec, lamb),  # Additional arguments passed to the cost function (target values and regularization parameter)\n",
    "    maxiter=100,  # Maximum number of iterations for the optimization\n",
    ")\n",
    "\n",
    "# Update model parameters with the optimized values\n",
    "unpack(optimal_theta)\n",
    "\n",
    "# Output results\n",
    "print(f\"Optimized alpha: {alpha}\")\n",
    "print(f\"Final MSE: {optimal_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You're all done!\n",
    "\n",
    "Congratulations! This project was the end of 4 whole courses worth of content! This project clearly didn't cover every single topic from those courses, but it serves as a summary for everything you have learned. This is only the start of Python Data Projects, so continue to learn and good luck in your future endeavors!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
